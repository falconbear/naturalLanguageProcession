{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19266043_final_report.ipynb ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 準備"
      ],
      "metadata": {
        "id": "djcNDl6rVR1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "googledriveのマウント"
      ],
      "metadata": {
        "id": "9nstuUOglXZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXtDYPuBSez3",
        "outputId": "b3723493-b2b2-46d2-dffd-c70ff36703bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MeCabのインストール"
      ],
      "metadata": {
        "id": "1qcuaK6OVv5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "id": "0HmRwQBu3eus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69abfd5-a9df-467f-f5a6-1f860d36d590"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.4\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 2.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=2ffb253145c207a831f1367ef871d13f8d36473ad77597b023ada83d0a636586\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tarファイルの展開"
      ],
      "metadata": {
        "id": "zwe7oS78lad_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "tar = tarfile.open('/content/drive/MyDrive/ldcc-20140209.tar.gz')\n",
        "tar.extractall('/content/drive/MyDrive/natural_language_processing')\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "WPIe95uPPdkp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "955ab227-d4b5-4589-c1dd-25462a6424ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0f000566f462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ldcc-20140209.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/natural_language_processing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0;32m-> 2002\u001b[0;31m                          numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;31m# Reverse sort directories.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2042\u001b[0m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[1;32m   2043\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                                  numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopybufsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mbltn_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 文書の下処理"
      ],
      "metadata": {
        "id": "b2EHNcI_jHZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "全ての形態素を読み込む"
      ],
      "metadata": {
        "id": "fZRKWeQjY6RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import MeCab\n",
        "path = \"/content/drive/MyDrive/natural_language_processing/text/\""
      ],
      "metadata": {
        "id": "gwdLAIyVLjGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories1 = []\n",
        "article_list1 = []\n",
        "labels1 = []\n",
        "\n",
        "#カテゴリ名の取得\n",
        "for dir_path1 in os.listdir(path):\n",
        "  #text配下に指定のディレクトリが存在する場合、categoriesにリストを追加\n",
        "  if os.path.isdir(os.path.join(path, dir_path1)):\n",
        "    categories1.append(dir_path1)\n",
        "\n",
        "#カテゴリごとの記事の取得\n",
        "for category1 in categories1:\n",
        "  #カテゴリー毎にディレクトリ配下にあるファイル一覧をarticlesに格納\n",
        "    articles1 = os.listdir(path+category1)\n",
        "\n",
        "    for article1 in articles1:\n",
        "      #記事の一覧からそれぞれの記事をtextにリスト化\n",
        "        with open(path+ category1+ \"/\"+article1, encoding=\"utf-8\") as f:\n",
        "            next(f)\n",
        "            next(f)\n",
        "            text1 = f.read()\n",
        "\n",
        "            #text1をarticle_listに追加\n",
        "            article_list1.append(text1)\n",
        "            #categoryのインデックスをlabelsに追加\n",
        "            labels1.append(categories1.index(category1))"
      ],
      "metadata": {
        "id": "NL9GFeH2dgrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動詞、名詞、形容詞を抽出して読み込む"
      ],
      "metadata": {
        "id": "Te5_nmmYZKPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories2 = []\n",
        "article_list2 = []\n",
        "labels2 = []\n",
        "select_conditions = ['動詞', '名詞','形容詞']\n",
        "\n",
        "# 分かち書きオブジェクト\n",
        "tagger = MeCab.Tagger('')\n",
        "tagger.parse('')\n",
        "\n",
        "#カテゴリ名の取得\n",
        "for dir_path2 in os.listdir(path):\n",
        "  if os.path.isdir(os.path.join(path, dir_path2)):\n",
        "    categories2.append(dir_path2)\n",
        "\n",
        "#カテゴリごとの記事の取得\n",
        "for category2 in categories2:\n",
        "    articles2 = os.listdir(path+category2)\n",
        "\n",
        "    for article2 in articles2:\n",
        "        with open(path+ category2+ \"/\"+article2, encoding=\"utf-8\") as f:\n",
        "            next(f)\n",
        "            next(f)\n",
        "            text2 = f.read()\n",
        "\n",
        "            #最初のnodeを取得\n",
        "            node = tagger.parseToNode(text2)\n",
        "            morphemes = []\n",
        "            while node:\n",
        "              #形態素をmorphomeに格納\n",
        "              morpheme = node.surface\n",
        "              #文字の特徴を','で区切った先頭要素（品詞）をposに格納\n",
        "              pos = node.feature.split(',')[0]\n",
        "\n",
        "              #品詞がselect_conditionsに含まれるノードのみmorphomesに形態素のリストを追加する\n",
        "              if pos in select_conditions:\n",
        "                  morphemes.append(morpheme)\n",
        "\n",
        "              #次のノードに移る\n",
        "              node = node.next\n",
        "\n",
        "            #morphomesの要素を半角スペースで分割\n",
        "            text_result = ' '.join(morphemes)\n",
        "\n",
        "            #text_resultをarticle_listに追加\n",
        "            article_list2.append(text_result)\n",
        "            #categoryのインデックスをlabelsに追加\n",
        "            labels2.append(categories2.index(category2))"
      ],
      "metadata": {
        "id": "7TAVhSBgS-zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "テストデータとの分割"
      ],
      "metadata": {
        "id": "nflB8wVadYCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "traindata1, testdata1, train_labels1, test_labels1 = train_test_split(article_list1, labels1, test_size=0.25, random_state=42)\n",
        "traindata2, testdata2, train_labels2, test_labels2 = train_test_split(article_list2, labels2, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "pjUQy8l6yPRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 文章のベクトル化"
      ],
      "metadata": {
        "id": "ELVRlKZBi9wK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag_of_wordsでのベクトル分割"
      ],
      "metadata": {
        "id": "bXxBOewidbZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#'(?u)\\\\b\\\\w+\\\\b'=“単語の境界\",\"1文字以上の単語構成文字\",\"単語の境界”\n",
        "vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "train_dataB1 = vectorizer.fit_transform(traindata1)\n",
        "test_dataB1 = vectorizer.transform(testdata1)\n",
        "train_dataB2 = vectorizer.fit_transform(traindata2)\n",
        "test_dataB2 = vectorizer.transform(testdata2)"
      ],
      "metadata": {
        "id": "gFET-YUsfV0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF*IDFでのベクトル分割\n",
        "\n"
      ],
      "metadata": {
        "id": "CuY1PlVIdgcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#'(?u)\\\\b\\\\w+\\\\b'=“単語の境界\",\"1文字以上の単語構成文字\",\"単語の境界”\n",
        "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "train_dataT1 = vectorizer.fit_transform(traindata1)\n",
        "test_dataT1 = vectorizer.transform(testdata1)\n",
        "train_dataT2 = vectorizer.fit_transform(traindata2)\n",
        "test_dataT2 = vectorizer.transform(testdata2)"
      ],
      "metadata": {
        "id": "jIHhjqg0s2hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 分類器の作成と検証"
      ],
      "metadata": {
        "id": "rmxZBlJ_iwNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダムフォレスト"
      ],
      "metadata": {
        "id": "bTsBMK1Hc4Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "\n",
        "score_rfc = []\n",
        "\n",
        "def result(a, b, c, d):\n",
        "  cl = rfc(n_estimators=10)\n",
        "  cl.fit(a, b)\n",
        "  score = cl.score(c, d)\n",
        "  return score\n",
        "\n",
        "score_rfc.append(result(train_dataB1, train_labels1, test_dataB1, test_labels1))\n",
        "score_rfc.append(result(train_dataT1, train_labels1, test_dataT1, test_labels1))\n",
        "score_rfc.append(result(train_dataB2, train_labels2, test_dataB2, test_labels2))\n",
        "score_rfc.append(result(train_dataT2, train_labels2, test_dataT2, test_labels2))\n",
        "\n",
        "score_rfc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_tX-U2Tyo_T",
        "outputId": "82156de3-0573-491a-c990-4824f2d9b26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8557483731019523,\n",
              " 0.8486984815618221,\n",
              " 0.8432754880694143,\n",
              " 0.8351409978308026]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ナイーブベイズ"
      ],
      "metadata": {
        "id": "zgdcPjuVc7Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB as NB\n",
        "\n",
        "score_NB = []\n",
        "\n",
        "def result(a, b, c, d):\n",
        "  cl_hard =  MultinomialNB()\n",
        "  cl_hard.fit(a, b)\n",
        "  score = cl_hard.score(c, d)\n",
        "  return score\n",
        "\n",
        "score_NB.append(result(train_dataB1, train_labels1, test_dataB1, test_labels1))\n",
        "score_NB.append(result(train_dataT1, train_labels1, test_dataT1, test_labels1))\n",
        "score_NB.append(result(train_dataB2, train_labels2, test_dataB2, test_labels2))\n",
        "score_NB.append(result(train_dataT2, train_labels2, test_dataT2, test_labels2))\n",
        "\n",
        "score_NB"
      ],
      "metadata": {
        "id": "KIQNShG5CCuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94947470-394c-47f4-c2eb-c306557ec5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9040130151843818,\n",
              " 0.8796095444685467,\n",
              " 0.8937093275488069,\n",
              " 0.8503253796095445]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ロジスティック回帰"
      ],
      "metadata": {
        "id": "SQG_SCwIc8uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "\n",
        "score_LR = []\n",
        "\n",
        "def result(a, b, c, d):\n",
        "  cl = LR(multi_class='multinomial', solver='newton-cg')\n",
        "  cl.fit(a, b)\n",
        "  score = cl.score(c, d)\n",
        "  return score\n",
        "\n",
        "score_LR.append(result(train_dataB1, train_labels1, test_dataB1, test_labels1))\n",
        "score_LR.append(result(train_dataT1, train_labels1, test_dataT1, test_labels1))\n",
        "score_LR.append(result(train_dataB2, train_labels2, test_dataB2, test_labels2))\n",
        "score_LR.append(result(train_dataT2, train_labels2, test_dataT2, test_labels2))\n",
        "\n",
        "score_LR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA3dzYIAYtKw",
        "outputId": "489320ba-c7bf-4dbc-a085-c2925a94e48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9148590021691974, 0.9018438177874186, 0.9408893709327549, 0.911062906724512]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "korc0p2Dc_c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC as svc\n",
        "\n",
        "score_SVC = []\n",
        "\n",
        "def result(a, b, c, d):\n",
        "  cl_soft=svc(loss='hinge')\n",
        "  cl_soft.fit(a, b)\n",
        "  score = cl_soft.score(c, d)\n",
        "  return score\n",
        "\n",
        "score_SVC.append(result(train_dataB1, train_labels1, test_dataB1, test_labels1))\n",
        "score_SVC.append(result(train_dataT1, train_labels1, test_dataT1, test_labels1))\n",
        "score_SVC.append(result(train_dataB2, train_labels2, test_dataB2, test_labels2))\n",
        "score_SVC.append(result(train_dataT2, train_labels2, test_dataT2, test_labels2))\n",
        "\n",
        "score_SVC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGG5p1EYZBqt",
        "outputId": "fec48885-ba12-423b-b5bc-86e9d2abceaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9224511930585684,\n",
              " 0.9262472885032538,\n",
              " 0.9419739696312365,\n",
              " 0.9360086767895879]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"～Bag_of_words～\")\n",
        "print(\"ランダムフォレスト    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_rfc[0]))\n",
        "print(\"ランダムフォレスト[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_rfc[2]))\n",
        "print(\"ナイーブベイズ    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_NB[0]))\n",
        "print(\"ナイーブベイズ[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_NB[2]))\n",
        "print(\"ロジスティック回帰    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_LR[0]))\n",
        "print(\"ロジスティック回帰[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_LR[2]))\n",
        "print(\"SVM    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_SVC[0]))\n",
        "print(\"SVM[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_SVC[2]))\n",
        "\n",
        "print(\"\\n\\n～TF*IDF～\")\n",
        "print(\"ランダムフォレスト    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_rfc[1]))\n",
        "print(\"ランダムフォレスト[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_rfc[3]))\n",
        "print(\"ナイーブベイズ    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_NB[1]))\n",
        "print(\"ナイーブベイズ[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_NB[3]))\n",
        "print(\"ロジスティック回帰    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_LR[1]))\n",
        "print(\"ロジスティック回帰[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_LR[3]))\n",
        "print(\"SVM    ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_SVC[1]))\n",
        "print(\"SVM[名詞,動詞,形容詞]   ：\",end=\"\")\n",
        "print('{:.3g}'.format(score_SVC[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZugOE9vqa5E",
        "outputId": "a1ed1327-97c7-4403-8842-0beeadc0f9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "～Bag_of_words～\n",
            "ランダムフォレスト    ：0.856\n",
            "ランダムフォレスト[名詞,動詞,形容詞]   ：0.843\n",
            "ナイーブベイズ    ：0.904\n",
            "ナイーブベイズ[名詞,動詞,形容詞]   ：0.894\n",
            "ロジスティック回帰    ：0.915\n",
            "ロジスティック回帰[名詞,動詞,形容詞]   ：0.941\n",
            "SVM    ：0.922\n",
            "SVM[名詞,動詞,形容詞]   ：0.942\n",
            "\n",
            "\n",
            "～TF*IDF～\n",
            "ランダムフォレスト    ：0.849\n",
            "ランダムフォレスト[名詞,動詞,形容詞]   ：0.835\n",
            "ナイーブベイズ    ：0.88\n",
            "ナイーブベイズ[名詞,動詞,形容詞]   ：0.85\n",
            "ロジスティック回帰    ：0.902\n",
            "ロジスティック回帰[名詞,動詞,形容詞]   ：0.911\n",
            "SVM    ：0.926\n",
            "SVM[名詞,動詞,形容詞]   ：0.936\n"
          ]
        }
      ]
    }
  ]
}